Here is the translation of the provided text from Hindi to English:

---

**(1) How to get OpenAI & Claude API for FREE | Unlimited Usage | o1, Claude 3.5 Sonnet, GPT-4o | Part-2 - YouTube**

- **00:00**: Disclaimer for the use under Section 107 of the Copyright Act.  
- **00:02**: Profit, educational, and personal purposes only.  
- **00:04**: Criticism, comments, and research. So, hello!  
- **00:05**: And welcome to a new video, guys. I hope...  
- **00:07**: You all are doing well, and for the love you showed in the last...  
- **00:09**: Video, thank you so much. That video has become the most viewed video on my channel so far...  
- **00:14**: And it's all because of you. As a result, I am back with a new video...  
- **00:17**: Where the free tier has been extended further, and more new models...  
- **00:20**: Have been added, along with new features and functionalities...  
- **00:22**: Many things have changed. So, make sure you watch the entire video, otherwise, like last time...  
- **00:27**: You'll be crying in the comments again, saying, "Bro, this isn't working, bro, that isn't working...  
- **00:30**: This model is down, I'm getting this error, what's wrong with the API?" So, it's better to take some time...  
- **00:34**: And watch the video completely. Otherwise, you'll face the same issues as before.  

- **01:00**: Some APIs cost $10, some $5, but in OpenAI, bro, $100 can go easily.  
- **01:03**: For a single user, it might be small and affordable, but if you scale up, bro, someone's $60 is going...  
- **01:10**: Like, what are you doing? I don't know. If you want to use it properly, you might get bankrupt without even realizing it.  
- **01:14**: So, it's much better to find an alternative where you can use the official OpenAI API for free...  
- **01:18**: Without any rate limits or the need to pay even a single rupee. You can use it unlimitedly for personal use.  
- **01:23**: So, let's move to our VS Code, where I'll explain everything properly.  

- **01:29**: So, we are now in VS Code, and the first thing I'll do is show you the previous API I was talking about in the last video.  
- **01:36**: For those who are new to my channel and haven't seen the previous video, don't worry. The video will start from scratch for you.  
- **01:43**: You don't need to worry at all. And for our previous audience who have seen the last video, there's a disclaimer...  
- **01:50**: The base URL has changed, and the API has also changed. The reason for this is that adding new models, functionalities, and features...  
- **01:55**: Like image generation, function calling, vision, etc., was causing a lot of issues, so we changed the server.  
- **02:03**: That's why the domain has changed, and all the backup URLs from last time are now non-functional.  
- **02:08**: You can directly shift here, where the free tier is extended with all the new models.  

- **02:14**: Let me explain. First, I'll show you the usage from last time. This is the database from the 18th, and today is the 19th.  
- **02:19**: On the 18th, I allowed nine models to be used for free. If I look at the model usage, you can see the model names...  
- **02:24**: I gave access to so many models for free, and you can see the number of requests, input/output tokens, etc.  
- **02:30**: The total cost in dollars is around $16. What is this, guys? So much!  
- **02:35**: For example, look at Jamba Flash, which is a very cheap model. It charges $5 for a million tokens...  
- **02:42**: And it's very affordable. Only $170 was spent because it's a smaller, cheaper model.  
- **02:46**: But if you look at GPT-4, its pricing is much higher. It charges $30.  
- **02:50**: You can check the pricing yourself. GPT-4o is $5, GPT-4 is $30.  
- **02:53**: So, even though the requests are fewer, the load is still high.  
- **02:58**: This time, instead of nine models, there are 40-50 models that you can check out gradually.  

- **03:05**: If I talk about total metrics, the total number of requests, successful requests, failed requests...  
- **03:10**: Some failed requests are due to server issues, and some are because users made mistakes in their requests.  
- **03:13**: Let's assume 2-3,000 are due to my mistakes, but the remaining 177,000 are because you guys didn't watch the video...  
- **03:19**: And made incorrect requests, and then came crying to me, saying, "Bro, this isn't working."  
- **03:23**: If I talk about token usage, so many tokens were used. I was looted, guys.  
- **03:27**: A total of $3,097 was spent, and it's expected that after this video, the cost will increase further.  
- **03:34**: There will be more misuse, but no problem, bro. If you show support, I'm ready to spend even more.  
- **03:39**: No issues at all.  

- **03:43**: Now, let's move to the free tier. This is a Jupyter Notebook file. If you don't know what that is, it's a very simple file.  
- **03:48**: No issues at all. You can run it like a normal Python file. My Python version is 3.1.10, but you can run it on any version.  
- **03:55**: It's compatible everywhere.  

- **04:00**: Now, here, I have a good number of followers and subscribers, but...  

- **04:29**: If you want to know more, you can copy this URL and come back to the code part.  
- **04:32**: Open a new tab and paste the URL. After that, you can add the "models" route, and you'll get a list of all the models available.  
- **04:39**: You can see which providers offer which models. Now, there's a slight change. You need to specify the provider.  
- **04:43**: I've divided things so that if one provider is down, you can check another provider with even more new and better models.  
- **04:48**: Provider 1 is the same as last time, with all the models I provided earlier.  
- **04:52**: Provider 2, Provider 3, Provider 4, Provider 5, etc., are all there.  
- **04:55**: There are models for image generation as well.  
- **04:58**: Provider 4 is the most unique provider, where you get everything—function calling, image generation, vision, etc.  
- **05:03**: Everything supported by OpenAI is available here.  

- **05:06**: Let's start from here. I've checked the models, and you can scroll through them.  
- **05:10**: You can check them out easily. No issues at all.  
- **05:13**: Provider 1 supports text generation and vision capabilities.  
- **05:16**: Let me run it. I asked a simple question: "Hello, how are you today?"  
- **05:19**: And here, you can see I got a lovely response. It wrote something long, but for now, we can ignore it.  
- **05:23**: If we talk about vision capabilities, here, I passed an image, and if I show it to you...  
- **05:30**: This is our image—a road with some grass on the side.  
- **05:34**: Our AI model gave a simple response: "The image shows a wooden product," etc.  
- **05:38**: It gave a detailed response, but we won't read it all.  
- **05:41**: It's an OpenAI-compatible payload. It's not like you can say, "Bro, this isn't OpenAI compatible."  
- **05:44**: It's fully OpenAI compatible. Just change the base API, and it will work perfectly.  

- **05:50**: Now, there's one more catch. For example, if I show you Provider 2, it supports text generation but not vision or function calling.  
- **05:54**: So, if I remove the provider and just write GPT-4, it will throw an error saying you need to specify the provider.  
- **06:05**: Otherwise, it won't work. So, make sure you specify the provider, and then you'll get a nice response.  

- **06:10**: Now, let's move to Provider 3. It also supports text generation. Currently, I'm running GPT-4 Mini here.  
- **06:16**: You can check other providers as well. Here, you can see which providers support which models.  
- **06:22**: If I show you a scrollable element, there are many things. I've added DeepSeek V3 this time, GPT-4o, Flash, Thinking, etc.  
- **06:28**: There are so many models. I'll show you properly so you can understand better.  

- **06:33**: If I run it on Provider 3, I asked it to tell me a short story about a brave knight, and I called GPT-4o Mini.  
- **06:37**: Here, you can see it wrote a story for me. It's an OpenAI-compatible response.  
- **06:42**: I haven't parsed it further, but you can parse it if you want. It's up to you.  

- **06:47**: Now, let's move to Provider 4, which is the most interesting. It supports text generation, vision capabilities, function calling, etc.  
- **06:52**: It has support for many models, but since it offers everything, there are some rate limits.  
- **06:55**: Because I can't let you use it unlimitedly for free. I'm using the official OpenAI backend, so I have to pay for it.  
- **07:00**: I'm not lying; I'm using the official OpenAI API in the backend.  
- **07:03**: So, you'll get the most stable uptime here, the lowest response time, and all the functionalities supported by OpenAI.  

- **07:12**: First, let's ask a simple question: "What is the capital of France?"  
- **07:15**: A simple question, nothing too hard. Then, for vision, I passed the same image as before.  
- **07:20**: I asked, "What is in this image?" and here, you can see I got a nice response.  
- **07:25**: It wrote something similar to what it explained last time.  

- **07:29**: Now, here's the function calling part. You can pass functions like this. It's not too complicated.  
- **07:33**: Most probably, function calling works like this. I haven't tested it fully, but I copied and pasted it.  
- **07:38**: I don't know why, but here, I got an error. I don't know why. When I tested it earlier, it was working.  
- **07:45**: Maybe I passed the wrong payload, but it does support function calling. You can try it.  
- **07:49**: Maybe I made a mistake, so it's not working now. But you can try it; it supports function calling.  
- **07:54**: When I tested it earlier, it was working. Maybe there's an issue now because I changed something while writing the comments.  
- **08:00**: So, you can check it out yourself. It supports function calling.  

- **08:06**: Now, let's move to Provider 5, which is the official OpenAI.  
- **08:09**: It has some rate limits because if you want to use it unlimitedly, I have to pay for it.  
- **08:12**: I can't let you use it unlimitedly for free because I have to pay real money to provide this stable experience.  
- **08:17**: So, there are some rate limits, but for normal users, it's unlimited.  
- **08:21**: You can use it unlimitedly. I haven't put any restrictions anywhere.  

- **08:25**: For example, on Provider 5, I'm using GPT-3.5 Mini. You can check which models are available on Provider 5.  
- **08:29**: I have no idea, but here, you can see I got a response in under a second. It was very fast.  
- **08:34**: Here, it wrote something like, "Of course, Paris is one of the busiest cities," etc.  
- **08:38**: I asked it something, and it gave a response.  

- **08:44**: Now, let's see the flux models. The image generation part is coming up.  
- **08:48**: While it's running, let me tell you that here, you'll find many models.  
- **08:54**: If I talk about image generation models, there's Flux Dev, Eddy Excel Turbo, and Flux Canal.  
- **08:58**: For now, that's it. If this video gets good views and I get support, more new models will be added.  
- **09:02**: Like F 1.1, Pro.rs 1.0, etc. You'll get all of that for free.  
- **09:11**: I'm not charging anywhere. The API I'm providing is completely free.  

- **09:15**: Now, here, I got an error. When I was testing, everything was working perfectly.  
- **09:18**: Now, I don't know why there's an issue. Most probably, when this video goes live, everything will be fine.  
- **09:23**: I just deployed it, so there might be some issues.  
- **09:25**: But you can test everything else. Maybe a particular model is down or something like that.  

- **09:29**: For status, let me show you. Here's our status.  
- **09:34**: You can see the health part. When I was testing, it was down for two hours.  
- **09:39**: So, you can count from today how much uptime it has.  
- **09:42**: Health means whether the server is healthy or not. If the server is down, all providers will be down.  
- **09:48**: If you want to test a specific provider, you can check each provider's uptime, which is above 99%.  
- **09:54**: The red bar doesn't mean it was down all day. It just means that on that day, something was down.  
- **09:58**: For example, the API was down for two minutes that day.  
- **10:01**: The official OpenAI, which is Provider 4, has 100% uptime because it's the official OpenAI.  
- **10:04**: So, if the official OpenAI goes down or has any issues, you'll notice it in the uptime.  
- **10:08**: Everything else is perfect. You can count from today.  
- **10:15**: Currently, it's in the testing phase, so don't count the health for now.  
- **10:18**: Today, it's perfectly operational because I just deployed it.  
- **10:21**: All providers are currently operational. If there's any issue with any provider on any day, you'll see it here.  

- **10:27**: If I talk about my previous video, for those who haven't seen it, no problem.  
- **10:29**: I'll explain it to them. Last time, I provided a free tier with nine models.  
- **10:33**: You can see the data for that. It has 96.6% uptime, which is more than enough.  
- **10:37**: If someone is providing you something for free, there's no issue.  
- **10:40**: And look, it was down for 7 minutes, 2 minutes, and 15 minutes.  
- **10:45**: But the rest of the days, it was fully functional at 100%.  
- **10:48**: So, it works like this: if the load is high on a particular day, it might go down.  
- **10:51**: But you'll get everything here.  

- **10:54**: I'll try to fix the Flux model now. I didn't just add it for no reason.  
- **10:57**: It was working during testing, but now it's throwing an error.  
- **10:59**: Let me run it again and see if it works.  
- **11:00**: It's taking time, so most probably, it's generating the image.  
- **11:04**: There shouldn't be an issue like an internal server error, but maybe there's some issue.  
- **11:09**: Look, the issue is from my side, not the server. Maybe there's some issue in the code.  
- **11:12**: It's not passing properly. You'll get the full source code, so don't worry.  

- **11:16**: Here, you can see so many models are available for free.  
- **11:20**: I've also mentioned the pricing for each model, so you don't need to pay anything.  
- **11:24**: I'll count based on the pricing. For example, if you use Jamba 2.0, I have to pay $5 per million tokens.  
- **11:29**: So, I'll count based on that. The approximate $4,000 bill I mentioned earlier is based on this.  

- **11:34**: On V1, there are so many models. Let me explain what V1 is.  
- **11:41**: On V1, you'll get Claude, GPT-4, etc. If you want to use Claude or Ada in the free tier, I can't provide that.  
- **11:47**: For example, Claude 3.5 Sonnet works perfectly, but it requires streaming.  
- **11:51**: On this API, you don't get streaming support because I can't provide it for free.  
- **11:55**: I'll go bankrupt. Claude consumes too many tokens, so I can't provide it for free.  
- **12:00**: I hope you understand.  

- **12:03**: If you want to shift and use Claude, you can come to our official V1 API.  
- **12:06**: V1 has a changed domain and endpoint. It's also OpenAI compatible, but the models have changed.  
- **12:12**: Here, you get over 100 models. You get GPT-3.5, GPT-4, GPT-4o, Claude, etc.  
- **12:17**: There's Claude 3.5 Sonnet, Jamba, and over 100 models. You can check them out easily.  
- **12:22**: No issues at all.  

- **12:25**: If I show you V1, it's also OpenAI compatible and supports streaming.  
- **12:29**: V2 is your official OpenAI API. Here, you get everything supported by OpenAI.  
- **12:34**: You also get access to OpenAI Preview and OpenAI Money.  
- **12:38**: The difference between OpenAI Preview and OpenAI Mini is that OpenAI is a more capable model.  
- **12:45**: It gives you the option to set the chain of thought—medium, high, low—depending on how much reasoning you want.  
- **12:51**: OpenAI Preview is a smaller version, which OpenAI has modified a lot recently.  
- **12:54**: It's up to you which one you want to use.  

- **13:01**: The best part of my API services is that I won't restrict you. You can use it unlimitedly.  
- **13:05**: I won't charge per token like OpenAI does, where you have to pay $5 per million tokens.  
- **13:10**: My model is a one-time payment for one month, and you get unlimited usage for personal use.  
- **13:15**: Remember, if you scale it for business, you'll have to pay separately.  
- **13:19**: You can contact me directly for that.  

- **13:21**: Otherwise, you can contact me on Instagram or any other platform.  
- **13:29**: It will support all platforms like Claude, Ada, etc. You just need to change the base URL and API.  
- **13:42**: There are no restrictions. You can contact me easily.  

- **13:49**: This is a paid API; you won't get it for free. So, make sure you don't message me asking for it for free.  
- **13:54**: I can't provide it for free. I don't have the resources to provide it for free.  
- **13:57**: If you want to use it for free, there's a free API with some limitations.  
- **14:00**: But if you want unlimited usage, no rate limits, dedicated servers, and client support, you can shift to the paid API.  
- **14:06**: Based on your usage, you can shift to V1 or V2 without any restrictions.  

- **14:10**: I hope you liked the video. If you did, please like, share, and subscribe.  
- **14:14**: If possible, comment in the description where I'll provide the base URL and API.  
- **14:18**: I'll also share all my social media handles and API keys.  
- **14:21**: You can contact me easily.  

- **14:25**: Make sure that if the last video did well, this video does even better.  
- **14:28**: If you want more extensions in the free tier or want to talk about something else...  
- **14:31**: On the 26th, which is Republic Day in India, I'm thinking of launching something big that benefits everyone.  
- **14:38**: A very extended free tier with some top-notch models and many surprises for you.  
- **14:42**: If everything goes well, we'll launch it on the 26th.  
- **14:44**: I just need your support. Like you showed support in the last video, show it for this video too.  
- **14:50**: And for the next video as well. The more support you show, the more free tier you'll get.  
- **14:55**: It's simple logic—nothing too complicated or hard.  

- **15:00**: If we talk about something else, there's a small help you can provide.  
- **15:04**: We need some developers to help us with new projects.  
- **15:07**: I'm working on many projects simultaneously—services, APIs, websites, etc.  
- **15:11**: So, if possible, please support us. If you can contribute a little, help us with coding or anything else.  
- **15:17**: I'm not asking for monetary contributions; I'm asking for your coding contributions.  
- **15:21**: If possible, please support us. Contact me through my social media handles.  
- **15:25**: We really need developers. If we're doing so much for you, please help us a little.  

- **15:30**: Based on that, I'll end the video here. I hope you liked the video.  
- **15:34**: If you did, please like, share, and subscribe. If you didn't like it, feel free to comment what you didn't like.  
- **15:38**: We can work on fixing those issues. We're building a good community where your support matters.  
- **15:44**: You point out our mistakes, and we bring new things with better experiences and approaches.  
- **15:49**: So, let's meet in the next video. Until then, show some love for this video.  
- **15:54**: And...  
- **15:58**: Bye-bye!

---

**Source:** [https://www.youtube.com/watch?v=Y_Pv7JVYBzM](https://www.youtube.com/watch?v=Y_Pv7JVYBzM)